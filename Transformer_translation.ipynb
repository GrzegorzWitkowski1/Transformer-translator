{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-4jGUkJClCE"
      },
      "source": [
        "# Transformer translation\n",
        "\n",
        "This implementation of neural network transformer translation from polish to english was bulit using the transformer architecture designed by google Tensorflow engineers (source below). I created the tokenizing and data processing functionality making it possible to input textual data in polsih and receive a properly parsed output translation in english. It does not handle proper names and has a limited vocabulary since the trainging data was not vast (40 000 columns of context-target sentence pairs). It will fail to translate longer sentences, so the approach to translating longer text it to split it into sentences, translate and correct punctuation.\n",
        "\n",
        "* Dataset: http://www.manythings.org/anki/\n",
        "* Model Architecture: https://www.tensorflow.org/text/tutorials/transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvNX5m5DClCJ"
      },
      "source": [
        "## Standard imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ9NuAzCKnWf",
        "outputId": "00ec79fa-e237-4f03-95a7-11413549421d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbEOLV37gT2k"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtslhPcIClCM"
      },
      "source": [
        "## Data processing functions and tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ExjqOZSJl4Y"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  path = Path(path)  # Create a Path object from the string path\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  triplets = [line.split('\\t') for line in lines] # context / target / source (source is not important)\n",
        "\n",
        "  context = []\n",
        "  target = []\n",
        "\n",
        "  for triplet in triplets:\n",
        "    target.append(triplet[0].strip())  # Extract the target and remove leading/trailing whitespaces\n",
        "    context.append(triplet[1].strip())  # Extract the context and remove leading/trailing whitespaces\n",
        "\n",
        "  context = np.array(context)\n",
        "  target = np.array(target)\n",
        "\n",
        "  return target, context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaEs90W8Jt8f"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Replace Polish letters with Latin letters.\n",
        "  text = tf.strings.regex_replace(text, '[łŁ]', 'l')\n",
        "  text = tf.strings.regex_replace(text, '[ąĄ]', 'a')\n",
        "  text = tf.strings.regex_replace(text, '[ćĆ]', 'c')\n",
        "  text = tf.strings.regex_replace(text, '[ęĘ]', 'e')\n",
        "  text = tf.strings.regex_replace(text, '[ńŃ]', 'n')\n",
        "  text = tf.strings.regex_replace(text, '[óÓ]', 'o') # perhaps it would be wiser to change óÓ to u\n",
        "  text = tf.strings.regex_replace(text, '[śŚ]', 's')\n",
        "  text = tf.strings.regex_replace(text, '[źŹ]', 'z')\n",
        "  text = tf.strings.regex_replace(text, '[żŻ]', 'z')\n",
        "  \n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-k_anbSK-jf"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbASERFXJxUN"
      },
      "outputs": [],
      "source": [
        "target_raw, context_raw = load_data(\"/content/pol.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di_HlWgZJ9WV"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(context_raw)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxEO5HCOKCrx"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "context_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                           max_tokens=max_vocab_size,\n",
        "                                                           ragged=True)\n",
        "\n",
        "target_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct,\n",
        "                                                          max_tokens=max_vocab_size,\n",
        "                                                          ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvWWAUfZKFcN"
      },
      "outputs": [],
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpNEo_xFKG62"
      },
      "outputs": [],
      "source": [
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaIafXpUClCR"
      },
      "outputs": [],
      "source": [
        "def split_sentences(text: str) -> list:\n",
        "    # Define the regex pattern to match sentence boundaries\n",
        "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
        "\n",
        "    # Split the text into sentences using the regex pattern\n",
        "    sentences = re.split(pattern, text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def sentence_correction(text: str) -> str:\n",
        "    text = text.capitalize()\n",
        "    text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiYGIkLbClCR"
      },
      "source": [
        "## Positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9X4RmO-gsYs"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length: int, depth: int) -> tf.Tensor:\n",
        "    depth = depth / 2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "\n",
        "    angle_rates = np.divide(1, np.power(10000, depths))\n",
        "    angle_rads = np.matmul(positions, angle_rates)\n",
        "\n",
        "    pos_encoding = np.concatenate(\n",
        "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "        axis = -1\n",
        "    )\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ-yUk3qhYFH"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        vocab_size,\n",
        "        d_model,\n",
        "        mask_zero=True\n",
        "    )\n",
        "    self.pos_encoding = positional_encoding(\n",
        "        length=2048,\n",
        "        depth=d_model\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxVVBuOClCS"
      },
      "source": [
        "## Attention layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E64UhnfSnvZm"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBe4JpugovTC"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True\n",
        "    )\n",
        "  \n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPDtO96ZpjiT"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x\n",
        "    )\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THHS6OwxqJ7m"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask=True\n",
        "    )\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j27GMtL2ClCU"
      },
      "source": [
        "## FFN with ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBkb0kP7qilH"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(d_model),\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHXRpR5_ClCU"
      },
      "source": [
        "## Encoder build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoaSedfnClCU"
      },
      "source": [
        "### Single layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S3TNmSWrazu"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.ffn = FeedForward(\n",
        "        d_model,\n",
        "        dff\n",
        "    )\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSTC7yJkClCV"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFeW2Q7rtFbx"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=d_model\n",
        "    )\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dff=dff,\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.pos_embedding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaP_ECp_ClCV"
      },
      "source": [
        "## Decoder build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo9MLW8aClCV"
      },
      "source": [
        "### Single layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dBMBgAk-Q8r"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, d_model, num_heads,\n",
        "               dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.ffn = FeedForward(\n",
        "        d_model,\n",
        "        dff\n",
        "    )\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x)\n",
        "    x = self.cross_attention(x, context)\n",
        "\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbwq7ouGClCW"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZURuuz6CnQw"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=d_model\n",
        "    )\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(\n",
        "        dropout_rate\n",
        "    )\n",
        "    \n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dff=dff,\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.pos_embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAyfplJNClCW"
      },
      "source": [
        "## Transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAS2TDaWFNBE"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        num_layers=num_layers, d_model=d_model,\n",
        "        num_heads=num_heads, dff=dff,\n",
        "        vocab_size=input_vocab_size,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        num_layers=num_layers, d_model=d_model,\n",
        "        num_heads=num_heads, dff=dff,\n",
        "        vocab_size=target_vocab_size,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "\n",
        "    context = self.encoder(context)\n",
        "\n",
        "    x = self.decoder(x, context)\n",
        "\n",
        "    logits = self.final_layer(x)\n",
        "\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0BwsYlwClCX"
      },
      "source": [
        "## Model build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIt1-PtLClCX"
      },
      "source": [
        "### hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmt3ggggG4iO"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzyKLfj8ClCi"
      },
      "source": [
        "### transformer initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOediEcOHBad"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=len(context_text_processor.get_vocabulary()),\n",
        "    target_vocab_size=len(target_text_processor.get_vocabulary()),\n",
        "    dropout_rate=dropout_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHFGjxqFClCi"
      },
      "source": [
        "### custom learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf40QakfMm5Z"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'d_model': self.d_model, 'warmup_steps': self.warmup_steps}\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y5FaQ5PPGAY"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YW1xdaRClCj"
      },
      "source": [
        "### loss function and accuracy metric definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRlQKEypPUAJ"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match_ = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match_ = match_ & mask\n",
        "\n",
        "  match_ = tf.cast(match_, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match_)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPAmfqEtPzr-"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxT74iOuClCk"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPUpw29pP26M",
        "outputId": "e00d67fe-cd08-4dbf-c424-d8589eef2ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "590/590 [==============================] - 117s 131ms/step - loss: 6.2531 - masked_accuracy: 0.2454 - val_loss: 4.2927 - val_masked_accuracy: 0.3564\n",
            "Epoch 2/20\n",
            "590/590 [==============================] - 44s 74ms/step - loss: 3.7100 - masked_accuracy: 0.4117 - val_loss: 3.1113 - val_masked_accuracy: 0.4799\n",
            "Epoch 3/20\n",
            "590/590 [==============================] - 43s 72ms/step - loss: 2.8481 - masked_accuracy: 0.5150 - val_loss: 2.4760 - val_masked_accuracy: 0.5664\n",
            "Epoch 4/20\n",
            "590/590 [==============================] - 43s 73ms/step - loss: 2.3083 - masked_accuracy: 0.5892 - val_loss: 2.1130 - val_masked_accuracy: 0.6275\n",
            "Epoch 5/20\n",
            "590/590 [==============================] - 43s 73ms/step - loss: 1.9664 - masked_accuracy: 0.6374 - val_loss: 1.9515 - val_masked_accuracy: 0.6445\n",
            "Epoch 6/20\n",
            "590/590 [==============================] - 42s 72ms/step - loss: 1.7539 - masked_accuracy: 0.6673 - val_loss: 1.8710 - val_masked_accuracy: 0.6563\n",
            "Epoch 7/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 1.6273 - masked_accuracy: 0.6834 - val_loss: 1.8149 - val_masked_accuracy: 0.6668\n",
            "Epoch 8/20\n",
            "590/590 [==============================] - 41s 70ms/step - loss: 1.4857 - masked_accuracy: 0.7030 - val_loss: 1.6654 - val_masked_accuracy: 0.6867\n",
            "Epoch 9/20\n",
            "590/590 [==============================] - 43s 73ms/step - loss: 1.3395 - masked_accuracy: 0.7243 - val_loss: 1.6259 - val_masked_accuracy: 0.6958\n",
            "Epoch 10/20\n",
            "590/590 [==============================] - 43s 72ms/step - loss: 1.2248 - masked_accuracy: 0.7415 - val_loss: 1.6225 - val_masked_accuracy: 0.6994\n",
            "Epoch 11/20\n",
            "590/590 [==============================] - 42s 72ms/step - loss: 1.1246 - masked_accuracy: 0.7565 - val_loss: 1.5834 - val_masked_accuracy: 0.7064\n",
            "Epoch 12/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 1.0371 - masked_accuracy: 0.7702 - val_loss: 1.5950 - val_masked_accuracy: 0.7063\n",
            "Epoch 13/20\n",
            "590/590 [==============================] - 41s 69ms/step - loss: 0.9637 - masked_accuracy: 0.7827 - val_loss: 1.6082 - val_masked_accuracy: 0.7061\n",
            "Epoch 14/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 0.8991 - masked_accuracy: 0.7933 - val_loss: 1.5855 - val_masked_accuracy: 0.7136\n",
            "Epoch 15/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 0.8431 - masked_accuracy: 0.8031 - val_loss: 1.6109 - val_masked_accuracy: 0.7126\n",
            "Epoch 16/20\n",
            "590/590 [==============================] - 43s 72ms/step - loss: 0.7878 - masked_accuracy: 0.8137 - val_loss: 1.6317 - val_masked_accuracy: 0.7155\n",
            "Epoch 17/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 0.7397 - masked_accuracy: 0.8216 - val_loss: 1.6582 - val_masked_accuracy: 0.7156\n",
            "Epoch 18/20\n",
            "590/590 [==============================] - 43s 72ms/step - loss: 0.6933 - masked_accuracy: 0.8316 - val_loss: 1.6536 - val_masked_accuracy: 0.7165\n",
            "Epoch 19/20\n",
            "590/590 [==============================] - 42s 71ms/step - loss: 0.6539 - masked_accuracy: 0.8392 - val_loss: 1.6849 - val_masked_accuracy: 0.7179\n",
            "Epoch 20/20\n",
            "590/590 [==============================] - 42s 72ms/step - loss: 0.6142 - masked_accuracy: 0.8468 - val_loss: 1.7154 - val_masked_accuracy: 0.7192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fca7d4964d0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "transformer.fit(train_ds,\n",
        "                epochs=20,\n",
        "                validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJIJ0lqKClCk"
      },
      "source": [
        "## Translator "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXcmuOxx_WiW"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 20\n",
        "\n",
        "class Translator(tf.Module):\n",
        "    def __init__(self, context_text_processor, target_text_processor, transformer):\n",
        "        self.context_tokenizer = context_text_processor\n",
        "        self.target_tokenizer = target_text_processor\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        assert isinstance(sentence, tf.Tensor)\n",
        "        if len(sentence.shape) == 0:\n",
        "            sentence = sentence[tf.newaxis]\n",
        "\n",
        "        sentence = self.context_tokenizer(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        start_end = self.target_tokenizer([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "            predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Terminate if the predicted ID is the end token.\n",
        "            if tf.reduce_all(tf.equal(predicted_id, end)):\n",
        "                break\n",
        "\n",
        "            # Write the predicted ID to the output array.\n",
        "            output_array = output_array.write(i + 1, predicted_id[0])\n",
        "\n",
        "        # Convert the output array to a tensor and remove the batch dimension.\n",
        "        output = output_array.stack()\n",
        "        output = tf.squeeze(output, axis=1)\n",
        "\n",
        "        # Convert token IDs to text.\n",
        "        predicted_sentence = [self.target_tokenizer.get_vocabulary()[token_id] for token_id in output.numpy()]\n",
        "\n",
        "        # Remove the start and end tokens from the predicted sentence.\n",
        "        predicted_sentence = predicted_sentence[1:-1]\n",
        "\n",
        "        return predicted_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9RUgmX1ClCl"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAzh8zOJRAq9"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    context_text_processor, target_text_processor, transformer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIKIZv0aR4Tw"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens):\n",
        "    print(f'{\"Input:\":15s}: {sentence}')\n",
        "    print(f'{\"Prediction\":15s}: {\" \".join(tokens)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUTKot_SwYfO"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"Cześć, jak się masz?\",\n",
        "    \"Gdzie jest najbliższy sklep spożywczy?\",\n",
        "    \"Jaki jest twój ulubiony kolor?\",\n",
        "    \"Ile masz lat?\",\n",
        "    \"Co chciałbyś zjeść na obiad?\",\n",
        "    \"Gdzie mieszkasz?\",\n",
        "    \"Czy możesz mi pomóc?\",\n",
        "    \"Dziękuję Ci za pomoc!\",\n",
        "    \"Jak się nazywasz?\",\n",
        "    \"Czy lubisz sport?\",\n",
        "    \"Czy możesz mi powiedzieć, która godzina?\",\n",
        "    \"Którego języka obcego chciałbyś nauczyć się?\",\n",
        "    \"Co robisz w wolnym czasie?\",\n",
        "    \"Jakie są twoje plany na weekend?\",\n",
        "    \"Gdzie można znaleźć dobre miejsce, aby wypić kawę?\",\n",
        "    \"Jakie są twoje zainteresowania?\",\n",
        "    \"Czy umiesz gotować?\",\n",
        "    \"Jakie jest twoje ulubione danie?\",\n",
        "    \"Co myślisz o polityce?\",\n",
        "    \"Czy masz rodzeństwo?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzRbfkuOvg44",
        "outputId": "1a32933a-284c-4bba-e6db-259ef0589d06",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Cześć, jak się masz?\n",
            "Prediction     : hi , how are you\n",
            "Input:         : Gdzie jest najbliższy sklep spożywczy?\n",
            "Prediction     : wheres the nearest store\n",
            "Input:         : Jaki jest twój ulubiony kolor?\n",
            "Prediction     : whats your favorite color\n",
            "Input:         : Ile masz lat?\n",
            "Prediction     : how old do you have\n",
            "Input:         : Co chciałbyś zjeść na obiad?\n",
            "Prediction     : what would you like to eat for dinner\n",
            "Input:         : Gdzie mieszkasz?\n",
            "Prediction     : where do you live\n",
            "Input:         : Czy możesz mi pomóc?\n",
            "Prediction     : can you help me\n",
            "Input:         : Dziękuję Ci za pomoc!\n",
            "Prediction     : thanks for helping you\n",
            "Input:         : Jak się nazywasz?\n",
            "Prediction     : how are you going to show up\n",
            "Input:         : Czy lubisz sport?\n",
            "Prediction     : do you like sports\n",
            "Input:         : Czy możesz mi powiedzieć, która godzina?\n",
            "Prediction     : can you tell me what time it is\n",
            "Input:         : Którego języka obcego chciałbyś nauczyć się?\n",
            "Prediction     : what language would you like to learn how to learn a foreign language\n",
            "Input:         : Co robisz w wolnym czasie?\n",
            "Prediction     : whatre you doing at the [UNK] time\n",
            "Input:         : Jakie są twoje plany na weekend?\n",
            "Prediction     : what are your plans for this weekend\n",
            "Input:         : Gdzie można znaleźć dobre miejsce, aby wypić kawę?\n",
            "Prediction     : where can you find good to drink to drink a cup of coffee\n",
            "Input:         : Jakie są twoje zainteresowania?\n",
            "Prediction     : what are your weight\n",
            "Input:         : Czy umiesz gotować?\n",
            "Prediction     : can you cook\n",
            "Input:         : Jakie jest twoje ulubione danie?\n",
            "Prediction     : whats your favorite holiday\n",
            "Input:         : Co myślisz o polityce?\n",
            "Prediction     : what are you thinking about\n",
            "Input:         : Czy masz rodzeństwo?\n",
            "Prediction     : do you have any brothers or sisters\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "  translated_text = translator(tf.constant(sentence))\n",
        "  print_translation(sentence, translated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}